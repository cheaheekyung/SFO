{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49542c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 🚀 SFO 프로젝트 피처엔지니어링 -dtw_clustering\n",
    "\n",
    "# - 원본 공통전처리 파일(baseline_processed.csv) 로드부터 `df_dtw` 생성\n",
    "# - 생성된 df_dtw을 csv로 저장\n",
    "#\n",
    "# [시작 전 준비사항]\n",
    "# 1. `baseline_processed.csv'  파일 경로 확인 (input_csv_file)\n",
    "# 2. 필요한 라이브러리 설치: `pip install pandas numpy scikit-learn ipykernel `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274e0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84241c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [1단계: dtw_clustering 피처 엔지니어링] ---\n",
      "1.1.공통 전처리 파일 로드 완료 (shape: (10624, 19))\n",
      "1.2 전체 고유 일자 수: 95\n",
      "    80% 위치 인덱스: 75\n",
      "    cutoff_date: 2022-04-22 (이 날짜까지 train으로 사용)\n",
      "1.3 train 기간: 2022-01-26 ~ 2022-04-22  (rows: 8,524)\n",
      "    test  기간: 2022-04-23 ~ 2022-05-11  (rows: 2,100)\n",
      "\n",
      "1.4 split 결과 미리보기 (상위 일부):\n",
      "  Product_Number       Date  T일 예정 수주량 __SPLIT__\n",
      "0     Product_84 2022-01-26          0     train\n",
      "1     Product_85 2022-01-26          0     train\n",
      "2     Product_86 2022-01-26        616     train\n",
      "3     Product_87 2022-01-26        104     train\n",
      "4     Product_88 2022-01-26        187     train\n",
      "5     Product_84 2022-04-23          3      test\n",
      "6     Product_85 2022-04-23         13      test\n",
      "7     Product_86 2022-04-23        559      test\n",
      "8     Product_87 2022-04-23         92      test\n",
      "9     Product_88 2022-04-23        239      test\n",
      "\n",
      "✅ 1단계 완료: df_train / df_test / cutoff_date 메모리에 준비되었습니다.\n",
      "   - df_train: train 데이터프레임 (DTW 클러스터링에 사용할 구간)\n",
      "   - df_test : 미래 데이터프레임 (cluster_id만 붙일 예정)\n",
      "   - cutoff_date: train/test 경계 날짜\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 0. 설정 및 파일 경로 정의 ---\n",
    "input_csv_file = '../../data/baseline_processed.csv'  \n",
    "output_final_csv_file = '../../data/dtw_clustering.csv' \n",
    "\n",
    "\n",
    "date_column = 'Date'\n",
    "product_column = 'Product_Number'\n",
    "target_column = 'T일 예정 수주량'\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- [1단계: dtw_clustering 피처 엔지니어링] ---\")\n",
    "\n",
    "df_dtw = None\n",
    "\n",
    "df_train = None\n",
    "df_test = None\n",
    "cutoff_date = None\n",
    "\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(input_csv_file):\n",
    "        raise FileNotFoundError(f\"🚨 오류: 입력 파일 '{input_csv_file}'을(를) 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "\n",
    "    df_raw = pd.read_csv(input_csv_file)\n",
    "    print(f\"1.1.공통 전처리 파일 로드 완료 (shape: {df_raw.shape})\")\n",
    "\n",
    "\n",
    "    # 필수 컬럼 존재 여부 확인\n",
    "    required_cols = {date_column, product_column, target_column}\n",
    "    missing_cols = required_cols - set(df_raw.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"🚨 필수 컬럼 {missing_cols} 이(가) 없습니다. CSV 컬럼명을 확인하세요.\")\n",
    "\n",
    "\n",
    "    # 날짜 컬럼 datetime 변환\n",
    "    df_raw[date_column] = pd.to_datetime(df_raw[date_column], errors='coerce')\n",
    "    if df_raw[date_column].isna().any():\n",
    "        bad_rows = df_raw[df_raw[date_column].isna()].head()\n",
    "        raise ValueError(\n",
    "            \"🚨 날짜 파싱 실패한 행이 있습니다. 일부 예시:\\n\"\n",
    "            + str(bad_rows)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # 5. 날짜/제품 기준 정렬 (시계열 안정성 확보)\n",
    "    df_raw = df_raw.sort_values(by=[date_column, product_column]).reset_index(drop=True)\n",
    "\n",
    "    # 6. cutoff_date 계산 (날짜 고유값 기준 80%)\n",
    "    unique_dates = pd.Series(df_raw[date_column].dropna().sort_values().unique())\n",
    "    total_days = len(unique_dates)\n",
    "\n",
    "    if total_days == 0:\n",
    "        raise ValueError(\"🚨 고유 날짜가 0입니다. 데이터셋이 비어있거나 Date 파싱이 잘못된 것 같습니다.\")\n",
    "\n",
    "    cut_index = int(total_days * 0.8) - 1  # 0-based index 보정\n",
    "    if cut_index < 0:\n",
    "        cut_index = 0\n",
    "    if cut_index >= total_days:\n",
    "        cut_index = total_days - 1\n",
    "\n",
    "    cutoff_date = unique_dates.iloc[cut_index]\n",
    "\n",
    "    print(f\"1.2 전체 고유 일자 수: {total_days}\")\n",
    "    print(f\"    80% 위치 인덱스: {cut_index}\")\n",
    "    print(f\"    cutoff_date: {cutoff_date.date()} (이 날짜까지 train으로 사용)\")\n",
    "\n",
    "    # 7. train / test 분리\n",
    "    df_train = df_raw[df_raw[date_column] <= cutoff_date].copy()\n",
    "    df_test  = df_raw[df_raw[date_column] >  cutoff_date].copy()\n",
    "\n",
    "    # 8. 누수 방지 점검: test에 train과 겹치는 날짜가 있으면 안 됨\n",
    "    if not df_test.empty:\n",
    "        if (df_test[date_column] <= df_train[date_column].max()).any():\n",
    "            raise RuntimeError(\n",
    "                \"🚨 누수 의심: test에 train보다 과거나 같은 날짜가 포함되어 있습니다. split 로직을 확인하세요.\"\n",
    "            )\n",
    "\n",
    "    # 9. split 결과 로그 출력\n",
    "    train_start = df_train[date_column].min().date() if not df_train.empty else None\n",
    "    train_end   = df_train[date_column].max().date() if not df_train.empty else None\n",
    "    test_start  = df_test[date_column].min().date()  if not df_test.empty else None\n",
    "    test_end    = df_test[date_column].max().date()  if not df_test.empty else None\n",
    "\n",
    "    print(f\"1.3 train 기간: {train_start} ~ {train_end}  (rows: {len(df_train):,})\")\n",
    "    print(f\"    test  기간: {test_start} ~ {test_end}  (rows: {len(df_test):,})\")\n",
    "\n",
    "    # 10. df_dtw 미리보기 (메모리 상에서만 확인)\n",
    "    df_train_preview = df_train[[product_column, date_column, target_column]].copy()\n",
    "    df_train_preview[\"__SPLIT__\"] = \"train\"\n",
    "\n",
    "    df_test_preview = df_test[[product_column, date_column, target_column]].copy()\n",
    "    df_test_preview[\"__SPLIT__\"] = \"test\"\n",
    "\n",
    "    df_preview = pd.concat([df_train_preview.head(), df_test_preview.head()], ignore_index=True)\n",
    "\n",
    "    print(\"\\n1.4 split 결과 미리보기 (상위 일부):\")\n",
    "    print(df_preview)\n",
    "\n",
    "    print(\"\\n✅ 1단계 완료: df_train / df_test / cutoff_date 메모리에 준비되었습니다.\")\n",
    "    print(\"   - df_train: train 데이터프레임 (DTW 클러스터링에 사용할 구간)\")\n",
    "    print(\"   - df_test : 미래 데이터프레임 (cluster_id만 붙일 예정)\")\n",
    "    print(\"   - cutoff_date: train/test 경계 날짜\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # df_dtw.to_csv(output_final_csv_file, index=False, encoding='utf-8-sig')\n",
    "    # print(f\"dtw_clustering 피처 엔지니어링 완료! (df_dtw shape: {df_dtw.shape})\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"🚨 {e}\")\n",
    "    df_time = None\n",
    "    df_train = None\n",
    "    df_test = None\n",
    "    cutoff_date = None\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"🚨 dtw_clustering 피처 엔지니어링 중 오류 발생: {e}\")\n",
    "    df_time = None\n",
    "    df_train = None\n",
    "    df_test = None\n",
    "    cutoff_date = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f89f358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [2단계: SKU별 주 단위 시계열 생성 & z-score 정규화 준비] ---\n",
      "2.4 SKU 개수 (train에서 관측된): 117\n",
      "2.5 유효 SKU(주 단위 3주 이상): 117개\n",
      "    관측 적어서 제외된 SKU: 0개 (예: [])\n",
      "2.7 z-score 변환 완료 (유효 SKU 117개)\n",
      "\n",
      "[샘플 SKU 1개 weekly 시계열 (원본 합계)]\n",
      "Date\n",
      "2022-01-30     0\n",
      "2022-02-06     0\n",
      "2022-02-13     0\n",
      "2022-02-20    12\n",
      "2022-02-27     8\n",
      "Freq: W-SUN, Name: T일 예정 수주량, dtype: int64\n",
      "\n",
      "[샘플 SKU 1개 weekly 시계열 (z-score 정규화)]\n",
      "Date\n",
      "2022-01-30   -0.646788\n",
      "2022-02-06   -0.646788\n",
      "2022-02-13   -0.646788\n",
      "2022-02-20    2.155959\n",
      "2022-02-27    1.221710\n",
      "Freq: W-SUN, Name: T일 예정 수주량, dtype: float64\n",
      "\n",
      "✅ 2단계 완료:\n",
      " - sku_weekly_sum: SKU별 주 단위 수요 합계 시계열 (train만 사용)\n",
      " - sku_weekly_z:   DTW용으로 z-score 정규화된 시계열 (길이 달라도 OK)\n",
      " - valid_skus:     DTW 클러스터링 후보 SKU 목록\n",
      " - lowdata_skus:   관측 적어서 cluster_id = -1 후보\n"
     ]
    }
   ],
   "source": [
    "# --- 2단계: train에서 SKU별 주 단위 시계열 생성 & 정규화 준비 ---\n",
    "\n",
    "\n",
    "print(\"\\n--- [2단계: SKU별 주 단위 시계열 생성 & z-score 정규화 준비] ---\")\n",
    "\n",
    "# 필수 전역 변수들이 존재하는지 확인\n",
    "needed_vars = [\"df_train\", \"cutoff_date\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals() or globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"🚨 '{var_name}' 이(가) 정의되어 있지 않습니다. 1단계 셀을 먼저 실행해야 합니다.\")\n",
    "\n",
    "# 우리가 이후에 계속 참조할 컬럼명들 (1단계와 동일하게 유지)\n",
    "date_column = 'Date'\n",
    "product_column = 'Product_Number'\n",
    "target_column = 'T일 예정 수주량'\n",
    "\n",
    "# 2.1 train 데이터만 사용 (안전하게 copy)\n",
    "work_df = df_train[[product_column, date_column, target_column]].copy()\n",
    "\n",
    "# 2.2 혹시라도 null target이 있으면 0으로 간주할지 / 드롭할지 결정\n",
    "#     여기서는 생산 계획 관점에서 \"기록이 있지만 수주량이 0\"과 \"기록 자체가 없음\"을 구분하고 싶으므로\n",
    "#     우선은 dropna() 하지 않고, 결측은 0으로 채우는 보수적 전략은 나중 단계에서 판단.\n",
    "#     우선은 결측이 있으면 경고만 띄우자.\n",
    "if work_df[target_column].isna().any():\n",
    "    null_count = work_df[target_column].isna().sum()\n",
    "    print(f\"⚠ 경고: train에서 '{target_column}'에 NaN {null_count}개 존재. NaN은 0으로 대체합니다.\")\n",
    "    work_df[target_column] = work_df[target_column].fillna(0)\n",
    "\n",
    "# 2.3 각 SKU별로 주 단위 시계열 만들기\n",
    "#     - resample('W')는 기본적으로 주 단위 끝(일요일 기준)로 집계\n",
    "#     - 공급/생산 측에서는 \"그 주에 총 얼마나 필요했나\"가 중요한 신호라 sum 사용\n",
    "sku_weekly_sum = {}\n",
    "\n",
    "for sku, sku_df in work_df.groupby(product_column):\n",
    "    # 개별 SKU만 뽑아서 시계열 정렬\n",
    "    sku_df = sku_df.sort_values(by=date_column)\n",
    "\n",
    "    # Date를 인덱스로 설정해서 주 단위 리샘플\n",
    "    sku_series = (\n",
    "        sku_df.set_index(date_column)[target_column]\n",
    "        .resample('W')  # 주 단위\n",
    "        .sum()          # 그 주의 총 예정 수주량\n",
    "    )\n",
    "\n",
    "    # 완전히 0만 있는 SKU도 일단 그대로 둠 (나중에 z-score에서 표준편차 0으로 처리)\n",
    "    sku_weekly_sum[sku] = sku_series\n",
    "\n",
    "print(f\"2.4 SKU 개수 (train에서 관측된): {len(sku_weekly_sum)}\")\n",
    "\n",
    "# 2.4 관측 길이가 너무 짧은 SKU는 제외 후보로 표시\n",
    "#     예: 관측 주차 수가 3주 미만이면 \"패턴\"이라고 부르기 어렵다고 가정\n",
    "min_weeks_required = 3\n",
    "\n",
    "valid_skus = []\n",
    "lowdata_skus = []\n",
    "\n",
    "for sku, series in sku_weekly_sum.items():\n",
    "    # 유효 관측 주(= 값이 전부 0이어도 주 단위로 존재하면 count로 잡힘)\n",
    "    observed_weeks = series.shape[0]\n",
    "    if observed_weeks < min_weeks_required:\n",
    "        lowdata_skus.append(sku)\n",
    "    else:\n",
    "        valid_skus.append(sku)\n",
    "\n",
    "print(f\"2.5 유효 SKU(주 단위 {min_weeks_required}주 이상): {len(valid_skus)}개\")\n",
    "print(f\"    관측 적어서 제외된 SKU: {len(lowdata_skus)}개 (예: {lowdata_skus[:5]})\")\n",
    "\n",
    "# 2.6 z-score 정규화 (valid_skus만)\n",
    "#     - 각 SKU 시계열을 자기 자신의 평균/표준편차로 정규화\n",
    "#     - 표준편차가 0이면 전부 0으로 (완전 안정 SKU로 간주)\n",
    "sku_weekly_z = {}\n",
    "\n",
    "for sku in valid_skus:\n",
    "    series = sku_weekly_sum[sku].astype(float)\n",
    "\n",
    "    mean_val = series.mean()\n",
    "    std_val = series.std(ddof=0)  # 모분산 기준 (ddof=0), 안정적으로 0 체크 가능\n",
    "\n",
    "    if std_val == 0 or np.isclose(std_val, 0.0):\n",
    "        z_series = pd.Series(\n",
    "            np.zeros_like(series.values, dtype=float),\n",
    "            index=series.index\n",
    "        )\n",
    "    else:\n",
    "        z_series = (series - mean_val) / std_val\n",
    "\n",
    "    sku_weekly_z[sku] = z_series\n",
    "\n",
    "print(f\"2.7 z-score 변환 완료 (유효 SKU {len(sku_weekly_z)}개)\")\n",
    "\n",
    "# 2.8 길이 표준화(패딩/트리밍)는 아직 안 함\n",
    "#     지금은 각 SKU별로 주차 길이가 다를 수 있음\n",
    "#     다음 단계(3단계)에서 DTW로 거리를 계산할 때, DTW는 길이가 달라도 괜찮으니까\n",
    "#     길이 맞출 필요 없이 그대로 distance matrix 계산 가능.\n",
    "\n",
    "# 2.9 요약 프린트\n",
    "example_sku = valid_skus[0] if valid_skus else None\n",
    "if example_sku is not None:\n",
    "    print(\"\\n[샘플 SKU 1개 weekly 시계열 (원본 합계)]\")\n",
    "    print(sku_weekly_sum[example_sku].head())\n",
    "\n",
    "    print(\"\\n[샘플 SKU 1개 weekly 시계열 (z-score 정규화)]\")\n",
    "    print(sku_weekly_z[example_sku].head())\n",
    "\n",
    "print(\"\\n✅ 2단계 완료:\")\n",
    "print(\" - sku_weekly_sum: SKU별 주 단위 수요 합계 시계열 (train만 사용)\")\n",
    "print(\" - sku_weekly_z:   DTW용으로 z-score 정규화된 시계열 (길이 달라도 OK)\")\n",
    "print(\" - valid_skus:     DTW 클러스터링 후보 SKU 목록\")\n",
    "print(\" - lowdata_skus:   관측 적어서 cluster_id = -1 후보\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b078db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [3단계: DTW 거리 행렬 계산] ---\n",
      "3.2 클러스터링 대상 SKU 수: 117\n",
      "3.5 DTW 거리 행렬 계산 완료.\n",
      "    dist_matrix shape: (117, 117)\n",
      "    dist_matrix[0:3, 0:3] 예시:\n",
      "[[ 0.          5.43735229 11.31886586]\n",
      " [ 5.43735229  0.         13.35943765]\n",
      " [11.31886586 13.35943765  0.        ]]\n",
      "3.6 대칭 여부: True\n",
      "    음수 거리 존재?: False\n",
      "\n",
      "✅ 3단계 완료:\n",
      " - sku_order: 거리행렬 인덱스와 SKU를 매핑할 리스트\n",
      " - dist_matrix: SKU 간 DTW 거리행렬 (shape = [N_SKU, N_SKU])\n",
      "이제 이걸 가지고 k 후보별 클러스터링과 실루엣 스코어를 구할 수 있습니다 (다음 셀에서 진행).\n"
     ]
    }
   ],
   "source": [
    "# --- 3단계: DTW 거리 행렬 계산 ---\n",
    "\n",
    "print(\"\\n--- [3단계: DTW 거리 행렬 계산] ---\")\n",
    "\n",
    "# 필수 전역 변수들이 있는지 체크\n",
    "needed_vars = [\"sku_weekly_z\", \"valid_skus\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals() or globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"🚨 '{var_name}' 이(가) 정의되어 있지 않습니다. 2단계 셀을 먼저 실행해야 합니다.\")\n",
    "\n",
    "# 3.1 DTW 함수 정의\n",
    "#     - 표준 동적 계획법 구현 (O(len(a)*len(b)))\n",
    "#     - 거리 = |a_i - b_j| 의 누적 최소 합 (L1 distance 기반)\n",
    "def dtw_distance(seq_a: np.ndarray, seq_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    seq_a, seq_b: 1D numpy arrays (float)\n",
    "    return: DTW alignment cost (float)\n",
    "    \"\"\"\n",
    "    len_a = len(seq_a)\n",
    "    len_b = len(seq_b)\n",
    "\n",
    "    # DP 매트릭스 초기화: 매우 큰 값으로 채움\n",
    "    dp = np.full((len_a + 1, len_b + 1), np.inf, dtype=float)\n",
    "    dp[0, 0] = 0.0\n",
    "\n",
    "    # 동적 계획: 삽입/삭제/대각선 이동 중 최소 비용 경로 누적\n",
    "    for i in range(1, len_a + 1):\n",
    "        ai = seq_a[i - 1]\n",
    "        for j in range(1, len_b + 1):\n",
    "            bj = seq_b[j - 1]\n",
    "            cost = abs(ai - bj)\n",
    "\n",
    "            dp[i, j] = cost + min(\n",
    "                dp[i - 1, j],     # 삭제 (a에서 한 걸음)\n",
    "                dp[i, j - 1],     # 삽입 (b에서 한 걸음)\n",
    "                dp[i - 1, j - 1]  # 매칭 (대각선)\n",
    "            )\n",
    "    return float(dp[len_a, len_b])\n",
    "\n",
    "# 3.2 sku_weekly_z에서 사용할 SKU 순서 고정\n",
    "#     - valid_skus는 우리가 2단계에서 관측 기간 충분하다고 판정한 SKU 목록\n",
    "sku_order = list(valid_skus)\n",
    "\n",
    "if len(sku_order) == 0:\n",
    "    raise RuntimeError(\"🚨 valid_skus가 비어 있습니다. 유효한 SKU가 없으면 클러스터링을 할 수 없습니다.\")\n",
    "\n",
    "print(f\"3.2 클러스터링 대상 SKU 수: {len(sku_order)}\")\n",
    "\n",
    "# 3.3 각 SKU의 시계열을 numpy array로 변환해서 캐시\n",
    "#     - 시계열 index(주차 날짜)는 DTW에서 직접 안 쓰고 값 벡터만 쓴다.\n",
    "sku_series_cache = {}\n",
    "for sku in sku_order:\n",
    "    series_z = sku_weekly_z[sku]\n",
    "    # series_z는 pandas Series (index=주별 날짜, values=z-score된 수요)\n",
    "    sku_series_cache[sku] = series_z.to_numpy(dtype=float)\n",
    "\n",
    "# 3.4 DTW 거리 행렬 계산\n",
    "n = len(sku_order)\n",
    "dist_matrix = np.zeros((n, n), dtype=float)\n",
    "\n",
    "for i in range(n):\n",
    "    seq_i = sku_series_cache[sku_order[i]]\n",
    "    for j in range(i + 1, n):\n",
    "        seq_j = sku_series_cache[sku_order[j]]\n",
    "\n",
    "        d = dtw_distance(seq_i, seq_j)\n",
    "        dist_matrix[i, j] = d\n",
    "        dist_matrix[j, i] = d\n",
    "\n",
    "# 대각선은 자기 자신이라 0 그대로 두면 됨\n",
    "\n",
    "print(\"3.5 DTW 거리 행렬 계산 완료.\")\n",
    "print(f\"    dist_matrix shape: {dist_matrix.shape}\")\n",
    "print(\"    dist_matrix[0:3, 0:3] 예시:\")\n",
    "print(dist_matrix[0:3, 0:3])\n",
    "\n",
    "# 3.6 간단 sanity check:\n",
    "#     - 대칭 여부\n",
    "#     - 음수 거리 없는지\n",
    "is_symmetric = np.allclose(dist_matrix, dist_matrix.T, atol=1e-12)\n",
    "has_negative = (dist_matrix < 0).any()\n",
    "\n",
    "print(f\"3.6 대칭 여부: {is_symmetric}\")\n",
    "print(f\"    음수 거리 존재?: {has_negative}\")\n",
    "\n",
    "if not is_symmetric:\n",
    "    raise RuntimeError(\"🚨 dist_matrix가 대칭이 아닙니다. DTW 계산 루프를 확인하세요.\")\n",
    "if has_negative:\n",
    "    raise RuntimeError(\"🚨 dist_matrix에 음수 거리가 있습니다. DTW distance 정의를 확인하세요.\")\n",
    "\n",
    "print(\"\\n✅ 3단계 완료:\")\n",
    "print(\" - sku_order: 거리행렬 인덱스와 SKU를 매핑할 리스트\")\n",
    "print(\" - dist_matrix: SKU 간 DTW 거리행렬 (shape = [N_SKU, N_SKU])\")\n",
    "print(\"이제 이걸 가지고 k 후보별 클러스터링과 실루엣 스코어를 구할 수 있습니다 (다음 셀에서 진행).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e90352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [4단계: k 최적화 및 cluster_map 생성 - Agglomerative v2] ---\n",
      "\n",
      "4.2 k 후보별 실루엣 점수:\n",
      "   k=2: silhouette=0.1330377594277968\n",
      "   k=3: silhouette=0.12959370372021664\n",
      "   k=4: silhouette=0.16314839049493565\n",
      "   k=5: silhouette=0.17174521232688753\n",
      "\n",
      "4.3 선택된 best_k = 5 (silhouette=0.17174521232688753)\n",
      "\n",
      "4.4 최종 cluster_map_df 예시 (상위 10개):\n",
      "  Product_Number  cluster_id\n",
      "0     Product_84           1\n",
      "1     Product_85           1\n",
      "2     Product_86           0\n",
      "3     Product_87           0\n",
      "4     Product_88           1\n",
      "5     Product_89           0\n",
      "6     Product_8a           0\n",
      "7     Product_8b           1\n",
      "8     Product_8c           1\n",
      "9     Product_8d           1\n",
      "\n",
      "✅ 4단계 완료:\n",
      " - best_k = 5, silhouette = 0.17174521232688753\n",
      " - cluster_map_df shape: (117, 2)\n",
      "   이제 cluster_map_df를 train/test 전체에 merge해서 cluster_id라는 범주형 피처를 붙일 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# --- 4단계: k 후보별 클러스터링 (Agglomerative v2), 실루엣 스코어 계산, best_k 선택, cluster_map 생성 ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print(\"\\n--- [4단계: k 최적화 및 cluster_map 생성 - Agglomerative v2] ---\")\n",
    "\n",
    "# 전 단계 산출물 필요\n",
    "needed_vars = [\"dist_matrix\", \"sku_order\", \"lowdata_skus\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals():\n",
    "        raise RuntimeError(f\"🚨 '{var_name}' 이(가) 정의되어 있지 않습니다. 이전 셀을 먼저 실행해야 합니다.\")\n",
    "    if globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"🚨 '{var_name}' 이(가) None 입니다. 이전 셀에서 생성이 안 된 것 같습니다.\")\n",
    "\n",
    "# 후보 k 값들\n",
    "k_candidates = [2, 3, 4, 5]\n",
    "\n",
    "silhouette_results = []\n",
    "label_results = {}\n",
    "\n",
    "for k in k_candidates:\n",
    "    if k >= len(sku_order):\n",
    "        print(f\"⚠ k={k} 는 SKU 수보다 많거나 같아서 스킵합니다.\")\n",
    "        continue\n",
    "\n",
    "    # AgglomerativeClustering (신형 sklearn API)\n",
    "    # - metric='precomputed' 로 DTW 거리행렬 사용\n",
    "    # - linkage='average' 로 군집 간 거리 계산\n",
    "    clusterer = AgglomerativeClustering(\n",
    "        n_clusters=k,\n",
    "        metric='precomputed',\n",
    "        linkage='average'\n",
    "    )\n",
    "\n",
    "    # fit_predict에 거리행렬(dist_matrix)을 그대로 넣는다.\n",
    "    labels = clusterer.fit_predict(dist_matrix)\n",
    "\n",
    "    # silhouette_score 계산\n",
    "    # silhouette_score(X, labels, metric='precomputed')\n",
    "    # X는 거리행렬\n",
    "    try:\n",
    "        sil = silhouette_score(dist_matrix, labels, metric=\"precomputed\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ k={k} 실루엣 계산 실패: {e}\")\n",
    "        sil = None\n",
    "\n",
    "    silhouette_results.append({\n",
    "        \"k\": k,\n",
    "        \"silhouette\": sil\n",
    "    })\n",
    "    label_results[k] = labels\n",
    "\n",
    "# 4.2 k별 실루엣 점수 출력\n",
    "print(\"\\n4.2 k 후보별 실루엣 점수:\")\n",
    "for res in silhouette_results:\n",
    "    print(f\"   k={res['k']}: silhouette={res['silhouette']}\")\n",
    "\n",
    "# 4.3 best_k 선택\n",
    "valid_scores = [\n",
    "    (res[\"k\"], res[\"silhouette\"])\n",
    "    for res in silhouette_results\n",
    "    if res[\"silhouette\"] is not None\n",
    "]\n",
    "\n",
    "if len(valid_scores) == 0:\n",
    "    raise RuntimeError(\"🚨 모든 k에 대해 실루엣 점수를 계산할 수 없습니다. 클러스터링이 실패했습니다.\")\n",
    "\n",
    "# 실루엣이 높은 k를 우선, 동률이면 더 작은 k를 선택\n",
    "best_k, best_sil = sorted(\n",
    "    valid_scores,\n",
    "    key=lambda x: (-x[1], x[0])\n",
    ")[0]\n",
    "\n",
    "print(f\"\\n4.3 선택된 best_k = {best_k} (silhouette={best_sil})\")\n",
    "\n",
    "best_labels = label_results[best_k]\n",
    "\n",
    "# 4.4 cluster_map_df 생성\n",
    "cluster_map_df = pd.DataFrame({\n",
    "    \"Product_Number\": sku_order,\n",
    "    \"cluster_id\": best_labels\n",
    "})\n",
    "\n",
    "# 관측 부족 SKU(-1) 처리 (lowdata_skus는 2단계에서 수집)\n",
    "if lowdata_skus:\n",
    "    low_df = pd.DataFrame({\n",
    "        \"Product_Number\": lowdata_skus,\n",
    "        \"cluster_id\": [-1] * len(lowdata_skus)\n",
    "    })\n",
    "    cluster_map_df = pd.concat([cluster_map_df, low_df], ignore_index=True)\n",
    "\n",
    "# 혹시라도 중복된 Product_Number가 생기면 마지막에 정리\n",
    "cluster_map_df = cluster_map_df.drop_duplicates(subset=[\"Product_Number\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n4.4 최종 cluster_map_df 예시 (상위 10개):\")\n",
    "print(cluster_map_df.head(10))\n",
    "\n",
    "print(f\"\\n✅ 4단계 완료:\")\n",
    "print(f\" - best_k = {best_k}, silhouette = {best_sil}\")\n",
    "print(f\" - cluster_map_df shape: {cluster_map_df.shape}\")\n",
    "print(\"   이제 cluster_map_df를 train/test 전체에 merge해서 cluster_id라는 범주형 피처를 붙일 수 있습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b05bfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [5단계: cluster_id 머지 & 최종 피처셋 생성] ---\n",
      "\n",
      "5.5 df_with_cluster 미리보기 (상위 5행):\n",
      "  Product_Number       Date  T일 예정 수주량  cluster_id __SPLIT__\n",
      "0     Product_84 2022-01-26          0           1     train\n",
      "1     Product_85 2022-01-26          0           1     train\n",
      "2     Product_86 2022-01-26        616           0     train\n",
      "3     Product_87 2022-01-26        104           0     train\n",
      "4     Product_88 2022-01-26        187           1     train\n",
      "\n",
      "5.6 cluster_id 분포 (value_counts):\n",
      "cluster_id\n",
      "1    4288\n",
      "0    3674\n",
      "2    1874\n",
      "4     760\n",
      "3      28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ 5단계 완료: 최종 피처셋을 '../../data/dtw_clustering.csv'로 저장했습니다.\n",
      "   최종 shape: (10624, 21)\n",
      "   이제 이 데이터를 가지고 LightGBM 실험 (baseline vs baseline+ts vs baseline+ts+cluster_id)으로 MAE 비교를 진행할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# --- 5단계: cluster_id를 train/test 전체에 merge하고 최종 CSV로 저장 ---\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- [5단계: cluster_id 머지 & 최종 피처셋 생성] ---\")\n",
    "\n",
    "# 전 단계 산출물 점검\n",
    "needed_vars = [\"df_train\", \"df_test\", \"cluster_map_df\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals() or globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"🚨 '{var_name}' 이(가) 비어있습니다. 이전 셀들을 먼저 실행하세요.\")\n",
    "\n",
    "date_column = 'Date'\n",
    "product_column = 'Product_Number'\n",
    "target_column = 'T일 예정 수주량'\n",
    "\n",
    "# 출력 경로 (최종 산출물)\n",
    "output_final_csv_file = '../../data/dtw_clustering.csv'\n",
    "\n",
    "# 5.1 train/test에 split 플래그 붙이기\n",
    "train_labeled = df_train.copy()\n",
    "train_labeled[\"__SPLIT__\"] = \"train\"\n",
    "\n",
    "test_labeled = df_test.copy()\n",
    "test_labeled[\"__SPLIT__\"] = \"test\"\n",
    "\n",
    "# 5.2 cluster_id merge (왼쪽 기준: train/test, 오른쪽 기준: cluster_map_df)\n",
    "train_merged = pd.merge(\n",
    "    train_labeled,\n",
    "    cluster_map_df,\n",
    "    how=\"left\",\n",
    "    left_on=product_column,\n",
    "    right_on=\"Product_Number\"\n",
    ")\n",
    "\n",
    "test_merged = pd.merge(\n",
    "    test_labeled,\n",
    "    cluster_map_df,\n",
    "    how=\"left\",\n",
    "    left_on=product_column,\n",
    "    right_on=\"Product_Number\"\n",
    ")\n",
    "\n",
    "# merge 후 중복된 Product_Number 컬럼 정리\n",
    "# (left_on=Product_Number / right_on=Product_Number 이라 동일 이름이 생길 수 있어서)\n",
    "if \"Product_Number_y\" in train_merged.columns:\n",
    "    # 정리: Product_Number_x -> Product_Number, drop Product_Number_y\n",
    "    train_merged.drop(columns=[\"Product_Number_y\"], inplace=True, errors=\"ignore\")\n",
    "    train_merged.rename(columns={\"Product_Number_x\": \"Product_Number\"}, inplace=True)\n",
    "\n",
    "if \"Product_Number_y\" in test_merged.columns:\n",
    "    test_merged.drop(columns=[\"Product_Number_y\"], inplace=True, errors=\"ignore\")\n",
    "    test_merged.rename(columns={\"Product_Number_x\": \"Product_Number\"}, inplace=True)\n",
    "\n",
    "# 5.3 cluster_id 누락 체크\n",
    "missing_train = train_merged[\"cluster_id\"].isna().sum()\n",
    "missing_test  = test_merged[\"cluster_id\"].isna().sum()\n",
    "\n",
    "if missing_train > 0 or missing_test > 0:\n",
    "    print(f\"⚠ 경고: cluster_id 매칭 안 된 행이 있습니다. train {missing_train}개, test {missing_test}개\")\n",
    "    # 이 상황이면 우리가 모르는 SKU가 test에 새로 등장했을 가능성.\n",
    "    # 그 SKU는 클러스터링 당시(train 기준) 패턴이 없었을 수도 있음.\n",
    "    # 이런 SKU는 cluster_id = -1으로 밀어넣는 게 안전하다.\n",
    "    train_merged[\"cluster_id\"] = train_merged[\"cluster_id\"].fillna(-1)\n",
    "    test_merged[\"cluster_id\"]  = test_merged[\"cluster_id\"].fillna(-1)\n",
    "\n",
    "# 5.4 최종 df 결합\n",
    "df_with_cluster = pd.concat([train_merged, test_merged], ignore_index=True)\n",
    "\n",
    "# 5.5 sanity check 프린트\n",
    "print(\"\\n5.5 df_with_cluster 미리보기 (상위 5행):\")\n",
    "print(df_with_cluster[[product_column, date_column, target_column, \"cluster_id\", \"__SPLIT__\"]].head())\n",
    "\n",
    "print(\"\\n5.6 cluster_id 분포 (value_counts):\")\n",
    "print(df_with_cluster[\"cluster_id\"].value_counts())\n",
    "\n",
    "# 5.7 저장\n",
    "# 이 df_with_cluster는 \"공통 전처리 + (기존 피처들) + cluster_id\"가 붙은 최종 피처셋이다.\n",
    "# 다음 단계(LightGBM)에서 바로 쓸 수 있다.\n",
    "os.makedirs(os.path.dirname(output_final_csv_file), exist_ok=True)\n",
    "df_with_cluster.to_csv(output_final_csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n✅ 5단계 완료: 최종 피처셋을 '{output_final_csv_file}'로 저장했습니다.\")\n",
    "print(f\"   최종 shape: {df_with_cluster.shape}\")\n",
    "print(\"   이제 이 데이터를 가지고 LightGBM 실험 (baseline vs baseline+ts vs baseline+ts+cluster_id)으로 MAE 비교를 진행할 수 있습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [5단계: cluster_id 머지 & 최종 피처셋 생성] ---\n",
      "\n",
      "5.5 df_with_cluster 미리보기 (상위 5행):\n",
      "   Product_Number       Date  T일 예정 수주량  cluster_id __SPLIT__\n",
      "0               0 2022-01-26          0           1     train\n",
      "1               1 2022-01-26          0           1     train\n",
      "2               2 2022-01-26        616           0     train\n",
      "3               3 2022-01-26        104           0     train\n",
      "4               4 2022-01-26        187           1     train\n",
      "\n",
      "5.6 cluster_id 분포 (value_counts):\n",
      "cluster_id\n",
      "1    4288\n",
      "0    3674\n",
      "2    1874\n",
      "4     760\n",
      "3      28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ 5단계 완료: 최종 피처셋을 '../../data/dtw_clustering.csv'로 저장했습니다.\n",
      "   최종 shape: (10624, 21)\n",
      "   이제 이 데이터를 이용해서 LightGBM 실험 (baseline vs cluster_id 포함)으로 test MAE 비교를 진행할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# --- 5단계: cluster_id를 train/test 전체에 merge하고 최종 CSV로 저장 ---\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- [5단계: cluster_id 머지 & 최종 피처셋 생성] ---\")\n",
    "\n",
    "# 전 단계 산출물 점검\n",
    "needed_vars = [\"df_train\", \"df_test\", \"cluster_map_df\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals() or globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"🚨 '{var_name}' 이(가) 비어있습니다. 이전 셀들을 먼저 실행하세요.\")\n",
    "\n",
    "date_column = 'Date'\n",
    "product_column = 'Product_Number'\n",
    "target_column = 'T일 예정 수주량'\n",
    "\n",
    "# 최종 산출 파일 경로\n",
    "output_final_csv_file = '../../data/dtw_clustering.csv'\n",
    "\n",
    "# 5.1 train/test에 split 플래그 붙이기\n",
    "train_labeled = df_train.copy()\n",
    "train_labeled[\"__SPLIT__\"] = \"train\"\n",
    "\n",
    "test_labeled = df_test.copy()\n",
    "test_labeled[\"__SPLIT__\"] = \"test\"\n",
    "\n",
    "# 5.2 cluster_id merge (왼쪽 = train/test, 오른쪽 = cluster_map_df)\n",
    "train_merged = pd.merge(\n",
    "    train_labeled,\n",
    "    cluster_map_df,\n",
    "    how=\"left\",\n",
    "    left_on=product_column,\n",
    "    right_on=\"Product_Number\"\n",
    ")\n",
    "\n",
    "test_merged = pd.merge(\n",
    "    test_labeled,\n",
    "    cluster_map_df,\n",
    "    how=\"left\",\n",
    "    left_on=product_column,\n",
    "    right_on=\"Product_Number\"\n",
    ")\n",
    "\n",
    "# 병합 후 중복되는 Product_Number_* 정리\n",
    "if \"Product_Number_y\" in train_merged.columns:\n",
    "    train_merged.drop(columns=[\"Product_Number_y\"], inplace=True, errors=\"ignore\")\n",
    "    train_merged.rename(columns={\"Product_Number_x\": \"Product_Number\"}, inplace=True)\n",
    "\n",
    "if \"Product_Number_y\" in test_merged.columns:\n",
    "    test_merged.drop(columns=[\"Product_Number_y\"], inplace=True, errors=\"ignore\")\n",
    "    test_merged.rename(columns={\"Product_Number_x\": \"Product_Number\"}, inplace=True)\n",
    "\n",
    "# 5.3 cluster_id 누락값 처리\n",
    "missing_train = train_merged[\"cluster_id\"].isna().sum()\n",
    "missing_test  = test_merged[\"cluster_id\"].isna().sum()\n",
    "\n",
    "if missing_train > 0 or missing_test > 0:\n",
    "    print(f\"⚠ 경고: cluster_id 매칭 안 된 행이 있습니다. train {missing_train}개, test {missing_test}개\")\n",
    "    # train 기간엔 원래 다 있어야 정상인데, test에는 신규 SKU가 노출될 수 있음.\n",
    "    # 그런 SKU는 cluster_id = -1로 태깅해서 '미분류/신규 리스크' 취급 가능.\n",
    "    train_merged[\"cluster_id\"] = train_merged[\"cluster_id\"].fillna(-1)\n",
    "    test_merged[\"cluster_id\"]  = test_merged[\"cluster_id\"].fillna(-1)\n",
    "\n",
    "# 5.4 최종 df 결합\n",
    "df_with_cluster = pd.concat([train_merged, test_merged], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# 5.5 sanity check 출력\n",
    "print(\"\\n5.5 df_with_cluster 미리보기 (상위 5행):\")\n",
    "print(df_with_cluster[[product_column, date_column, target_column, \"cluster_id\", \"__SPLIT__\"]].head())\n",
    "\n",
    "print(\"\\n5.6 cluster_id 분포 (value_counts):\")\n",
    "print(df_with_cluster[\"cluster_id\"].value_counts())\n",
    "\n",
    "# 5.7 저장\n",
    "os.makedirs(os.path.dirname(output_final_csv_file), exist_ok=True)\n",
    "df_with_cluster.to_csv(output_final_csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n✅ 5단계 완료: 최종 피처셋을 '{output_final_csv_file}'로 저장했습니다.\")\n",
    "print(f\"   최종 shape: {df_with_cluster.shape}\")\n",
    "print(\"   이제 이 데이터를 이용해서 LightGBM 실험 (baseline vs cluster_id 포함)으로 \"\n",
    "      \"test MAE 비교를 진행할 수 있습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
