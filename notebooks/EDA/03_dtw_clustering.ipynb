{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49542c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## ğŸš€ SFO í”„ë¡œì íŠ¸ í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§ -dtw_clustering\n",
    "\n",
    "# - ì›ë³¸ ê³µí†µì „ì²˜ë¦¬ íŒŒì¼(baseline_processed.csv) ë¡œë“œë¶€í„° `df_dtw` ìƒì„±\n",
    "# - ìƒì„±ëœ df_dtwì„ csvë¡œ ì €ì¥\n",
    "#\n",
    "# [ì‹œì‘ ì „ ì¤€ë¹„ì‚¬í•­]\n",
    "# 1. `baseline_processed.csv'  íŒŒì¼ ê²½ë¡œ í™•ì¸ (input_csv_file)\n",
    "# 2. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: `pip install pandas numpy scikit-learn ipykernel `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274e0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84241c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [1ë‹¨ê³„: dtw_clustering í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§] ---\n",
      "1.1.ê³µí†µ ì „ì²˜ë¦¬ íŒŒì¼ ë¡œë“œ ì™„ë£Œ (shape: (10624, 19))\n",
      "1.2 ì „ì²´ ê³ ìœ  ì¼ì ìˆ˜: 95\n",
      "    80% ìœ„ì¹˜ ì¸ë±ìŠ¤: 75\n",
      "    cutoff_date: 2022-04-22 (ì´ ë‚ ì§œê¹Œì§€ trainìœ¼ë¡œ ì‚¬ìš©)\n",
      "1.3 train ê¸°ê°„: 2022-01-26 ~ 2022-04-22  (rows: 8,524)\n",
      "    test  ê¸°ê°„: 2022-04-23 ~ 2022-05-11  (rows: 2,100)\n",
      "\n",
      "1.4 split ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ ì¼ë¶€):\n",
      "  Product_Number       Date  Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰ __SPLIT__\n",
      "0     Product_84 2022-01-26          0     train\n",
      "1     Product_85 2022-01-26          0     train\n",
      "2     Product_86 2022-01-26        616     train\n",
      "3     Product_87 2022-01-26        104     train\n",
      "4     Product_88 2022-01-26        187     train\n",
      "5     Product_84 2022-04-23          3      test\n",
      "6     Product_85 2022-04-23         13      test\n",
      "7     Product_86 2022-04-23        559      test\n",
      "8     Product_87 2022-04-23         92      test\n",
      "9     Product_88 2022-04-23        239      test\n",
      "\n",
      "âœ… 1ë‹¨ê³„ ì™„ë£Œ: df_train / df_test / cutoff_date ë©”ëª¨ë¦¬ì— ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "   - df_train: train ë°ì´í„°í”„ë ˆì„ (DTW í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  êµ¬ê°„)\n",
      "   - df_test : ë¯¸ë˜ ë°ì´í„°í”„ë ˆì„ (cluster_idë§Œ ë¶™ì¼ ì˜ˆì •)\n",
      "   - cutoff_date: train/test ê²½ê³„ ë‚ ì§œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 0. ì„¤ì • ë° íŒŒì¼ ê²½ë¡œ ì •ì˜ ---\n",
    "input_csv_file = '../../data/baseline_processed.csv'  \n",
    "output_final_csv_file = '../../data/dtw_clustering.csv' \n",
    "\n",
    "\n",
    "date_column = 'Date'\n",
    "product_column = 'Product_Number'\n",
    "target_column = 'Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰'\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- [1ë‹¨ê³„: dtw_clustering í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§] ---\")\n",
    "\n",
    "df_dtw = None\n",
    "\n",
    "df_train = None\n",
    "df_test = None\n",
    "cutoff_date = None\n",
    "\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(input_csv_file):\n",
    "        raise FileNotFoundError(f\"ğŸš¨ ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ '{input_csv_file}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "    df_raw = pd.read_csv(input_csv_file)\n",
    "    print(f\"1.1.ê³µí†µ ì „ì²˜ë¦¬ íŒŒì¼ ë¡œë“œ ì™„ë£Œ (shape: {df_raw.shape})\")\n",
    "\n",
    "\n",
    "    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    required_cols = {date_column, product_column, target_column}\n",
    "    missing_cols = required_cols - set(df_raw.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"ğŸš¨ í•„ìˆ˜ ì»¬ëŸ¼ {missing_cols} ì´(ê°€) ì—†ìŠµë‹ˆë‹¤. CSV ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "    # ë‚ ì§œ ì»¬ëŸ¼ datetime ë³€í™˜\n",
    "    df_raw[date_column] = pd.to_datetime(df_raw[date_column], errors='coerce')\n",
    "    if df_raw[date_column].isna().any():\n",
    "        bad_rows = df_raw[df_raw[date_column].isna()].head()\n",
    "        raise ValueError(\n",
    "            \"ğŸš¨ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•œ í–‰ì´ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì˜ˆì‹œ:\\n\"\n",
    "            + str(bad_rows)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # 5. ë‚ ì§œ/ì œí’ˆ ê¸°ì¤€ ì •ë ¬ (ì‹œê³„ì—´ ì•ˆì •ì„± í™•ë³´)\n",
    "    df_raw = df_raw.sort_values(by=[date_column, product_column]).reset_index(drop=True)\n",
    "\n",
    "    # 6. cutoff_date ê³„ì‚° (ë‚ ì§œ ê³ ìœ ê°’ ê¸°ì¤€ 80%)\n",
    "    unique_dates = pd.Series(df_raw[date_column].dropna().sort_values().unique())\n",
    "    total_days = len(unique_dates)\n",
    "\n",
    "    if total_days == 0:\n",
    "        raise ValueError(\"ğŸš¨ ê³ ìœ  ë‚ ì§œê°€ 0ì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆê±°ë‚˜ Date íŒŒì‹±ì´ ì˜ëª»ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    cut_index = int(total_days * 0.8) - 1  # 0-based index ë³´ì •\n",
    "    if cut_index < 0:\n",
    "        cut_index = 0\n",
    "    if cut_index >= total_days:\n",
    "        cut_index = total_days - 1\n",
    "\n",
    "    cutoff_date = unique_dates.iloc[cut_index]\n",
    "\n",
    "    print(f\"1.2 ì „ì²´ ê³ ìœ  ì¼ì ìˆ˜: {total_days}\")\n",
    "    print(f\"    80% ìœ„ì¹˜ ì¸ë±ìŠ¤: {cut_index}\")\n",
    "    print(f\"    cutoff_date: {cutoff_date.date()} (ì´ ë‚ ì§œê¹Œì§€ trainìœ¼ë¡œ ì‚¬ìš©)\")\n",
    "\n",
    "    # 7. train / test ë¶„ë¦¬\n",
    "    df_train = df_raw[df_raw[date_column] <= cutoff_date].copy()\n",
    "    df_test  = df_raw[df_raw[date_column] >  cutoff_date].copy()\n",
    "\n",
    "    # 8. ëˆ„ìˆ˜ ë°©ì§€ ì ê²€: testì— trainê³¼ ê²¹ì¹˜ëŠ” ë‚ ì§œê°€ ìˆìœ¼ë©´ ì•ˆ ë¨\n",
    "    if not df_test.empty:\n",
    "        if (df_test[date_column] <= df_train[date_column].max()).any():\n",
    "            raise RuntimeError(\n",
    "                \"ğŸš¨ ëˆ„ìˆ˜ ì˜ì‹¬: testì— trainë³´ë‹¤ ê³¼ê±°ë‚˜ ê°™ì€ ë‚ ì§œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. split ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "\n",
    "    # 9. split ê²°ê³¼ ë¡œê·¸ ì¶œë ¥\n",
    "    train_start = df_train[date_column].min().date() if not df_train.empty else None\n",
    "    train_end   = df_train[date_column].max().date() if not df_train.empty else None\n",
    "    test_start  = df_test[date_column].min().date()  if not df_test.empty else None\n",
    "    test_end    = df_test[date_column].max().date()  if not df_test.empty else None\n",
    "\n",
    "    print(f\"1.3 train ê¸°ê°„: {train_start} ~ {train_end}  (rows: {len(df_train):,})\")\n",
    "    print(f\"    test  ê¸°ê°„: {test_start} ~ {test_end}  (rows: {len(df_test):,})\")\n",
    "\n",
    "    # 10. df_dtw ë¯¸ë¦¬ë³´ê¸° (ë©”ëª¨ë¦¬ ìƒì—ì„œë§Œ í™•ì¸)\n",
    "    df_train_preview = df_train[[product_column, date_column, target_column]].copy()\n",
    "    df_train_preview[\"__SPLIT__\"] = \"train\"\n",
    "\n",
    "    df_test_preview = df_test[[product_column, date_column, target_column]].copy()\n",
    "    df_test_preview[\"__SPLIT__\"] = \"test\"\n",
    "\n",
    "    df_preview = pd.concat([df_train_preview.head(), df_test_preview.head()], ignore_index=True)\n",
    "\n",
    "    print(\"\\n1.4 split ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ ì¼ë¶€):\")\n",
    "    print(df_preview)\n",
    "\n",
    "    print(\"\\nâœ… 1ë‹¨ê³„ ì™„ë£Œ: df_train / df_test / cutoff_date ë©”ëª¨ë¦¬ì— ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   - df_train: train ë°ì´í„°í”„ë ˆì„ (DTW í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  êµ¬ê°„)\")\n",
    "    print(\"   - df_test : ë¯¸ë˜ ë°ì´í„°í”„ë ˆì„ (cluster_idë§Œ ë¶™ì¼ ì˜ˆì •)\")\n",
    "    print(\"   - cutoff_date: train/test ê²½ê³„ ë‚ ì§œ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # df_dtw.to_csv(output_final_csv_file, index=False, encoding='utf-8-sig')\n",
    "    # print(f\"dtw_clustering í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ! (df_dtw shape: {df_dtw.shape})\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ğŸš¨ {e}\")\n",
    "    df_time = None\n",
    "    df_train = None\n",
    "    df_test = None\n",
    "    cutoff_date = None\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ dtw_clustering í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    df_time = None\n",
    "    df_train = None\n",
    "    df_test = None\n",
    "    cutoff_date = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f89f358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [2ë‹¨ê³„: SKUë³„ ì£¼ ë‹¨ìœ„ ì‹œê³„ì—´ ìƒì„± & z-score ì •ê·œí™” ì¤€ë¹„] ---\n",
      "2.4 SKU ê°œìˆ˜ (trainì—ì„œ ê´€ì¸¡ëœ): 117\n",
      "2.5 ìœ íš¨ SKU(ì£¼ ë‹¨ìœ„ 3ì£¼ ì´ìƒ): 117ê°œ\n",
      "    ê´€ì¸¡ ì ì–´ì„œ ì œì™¸ëœ SKU: 0ê°œ (ì˜ˆ: [])\n",
      "2.7 z-score ë³€í™˜ ì™„ë£Œ (ìœ íš¨ SKU 117ê°œ)\n",
      "\n",
      "[ìƒ˜í”Œ SKU 1ê°œ weekly ì‹œê³„ì—´ (ì›ë³¸ í•©ê³„)]\n",
      "Date\n",
      "2022-01-30     0\n",
      "2022-02-06     0\n",
      "2022-02-13     0\n",
      "2022-02-20    12\n",
      "2022-02-27     8\n",
      "Freq: W-SUN, Name: Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰, dtype: int64\n",
      "\n",
      "[ìƒ˜í”Œ SKU 1ê°œ weekly ì‹œê³„ì—´ (z-score ì •ê·œí™”)]\n",
      "Date\n",
      "2022-01-30   -0.646788\n",
      "2022-02-06   -0.646788\n",
      "2022-02-13   -0.646788\n",
      "2022-02-20    2.155959\n",
      "2022-02-27    1.221710\n",
      "Freq: W-SUN, Name: Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰, dtype: float64\n",
      "\n",
      "âœ… 2ë‹¨ê³„ ì™„ë£Œ:\n",
      " - sku_weekly_sum: SKUë³„ ì£¼ ë‹¨ìœ„ ìˆ˜ìš” í•©ê³„ ì‹œê³„ì—´ (trainë§Œ ì‚¬ìš©)\n",
      " - sku_weekly_z:   DTWìš©ìœ¼ë¡œ z-score ì •ê·œí™”ëœ ì‹œê³„ì—´ (ê¸¸ì´ ë‹¬ë¼ë„ OK)\n",
      " - valid_skus:     DTW í´ëŸ¬ìŠ¤í„°ë§ í›„ë³´ SKU ëª©ë¡\n",
      " - lowdata_skus:   ê´€ì¸¡ ì ì–´ì„œ cluster_id = -1 í›„ë³´\n"
     ]
    }
   ],
   "source": [
    "# --- 2ë‹¨ê³„: trainì—ì„œ SKUë³„ ì£¼ ë‹¨ìœ„ ì‹œê³„ì—´ ìƒì„± & ì •ê·œí™” ì¤€ë¹„ ---\n",
    "\n",
    "\n",
    "print(\"\\n--- [2ë‹¨ê³„: SKUë³„ ì£¼ ë‹¨ìœ„ ì‹œê³„ì—´ ìƒì„± & z-score ì •ê·œí™” ì¤€ë¹„] ---\")\n",
    "\n",
    "# í•„ìˆ˜ ì „ì—­ ë³€ìˆ˜ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "needed_vars = [\"df_train\", \"cutoff_date\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals() or globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"ğŸš¨ '{var_name}' ì´(ê°€) ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 1ë‹¨ê³„ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ìš°ë¦¬ê°€ ì´í›„ì— ê³„ì† ì°¸ì¡°í•  ì»¬ëŸ¼ëª…ë“¤ (1ë‹¨ê³„ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)\n",
    "date_column = 'Date'\n",
    "product_column = 'Product_Number'\n",
    "target_column = 'Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰'\n",
    "\n",
    "# 2.1 train ë°ì´í„°ë§Œ ì‚¬ìš© (ì•ˆì „í•˜ê²Œ copy)\n",
    "work_df = df_train[[product_column, date_column, target_column]].copy()\n",
    "\n",
    "# 2.2 í˜¹ì‹œë¼ë„ null targetì´ ìˆìœ¼ë©´ 0ìœ¼ë¡œ ê°„ì£¼í• ì§€ / ë“œë¡­í• ì§€ ê²°ì •\n",
    "#     ì—¬ê¸°ì„œëŠ” ìƒì‚° ê³„íš ê´€ì ì—ì„œ \"ê¸°ë¡ì´ ìˆì§€ë§Œ ìˆ˜ì£¼ëŸ‰ì´ 0\"ê³¼ \"ê¸°ë¡ ìì²´ê°€ ì—†ìŒ\"ì„ êµ¬ë¶„í•˜ê³  ì‹¶ìœ¼ë¯€ë¡œ\n",
    "#     ìš°ì„ ì€ dropna() í•˜ì§€ ì•Šê³ , ê²°ì¸¡ì€ 0ìœ¼ë¡œ ì±„ìš°ëŠ” ë³´ìˆ˜ì  ì „ëµì€ ë‚˜ì¤‘ ë‹¨ê³„ì—ì„œ íŒë‹¨.\n",
    "#     ìš°ì„ ì€ ê²°ì¸¡ì´ ìˆìœ¼ë©´ ê²½ê³ ë§Œ ë„ìš°ì.\n",
    "if work_df[target_column].isna().any():\n",
    "    null_count = work_df[target_column].isna().sum()\n",
    "    print(f\"âš  ê²½ê³ : trainì—ì„œ '{target_column}'ì— NaN {null_count}ê°œ ì¡´ì¬. NaNì€ 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "    work_df[target_column] = work_df[target_column].fillna(0)\n",
    "\n",
    "# 2.3 ê° SKUë³„ë¡œ ì£¼ ë‹¨ìœ„ ì‹œê³„ì—´ ë§Œë“¤ê¸°\n",
    "#     - resample('W')ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì£¼ ë‹¨ìœ„ ë(ì¼ìš”ì¼ ê¸°ì¤€)ë¡œ ì§‘ê³„\n",
    "#     - ê³µê¸‰/ìƒì‚° ì¸¡ì—ì„œëŠ” \"ê·¸ ì£¼ì— ì´ ì–¼ë§ˆë‚˜ í•„ìš”í–ˆë‚˜\"ê°€ ì¤‘ìš”í•œ ì‹ í˜¸ë¼ sum ì‚¬ìš©\n",
    "sku_weekly_sum = {}\n",
    "\n",
    "for sku, sku_df in work_df.groupby(product_column):\n",
    "    # ê°œë³„ SKUë§Œ ë½‘ì•„ì„œ ì‹œê³„ì—´ ì •ë ¬\n",
    "    sku_df = sku_df.sort_values(by=date_column)\n",
    "\n",
    "    # Dateë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •í•´ì„œ ì£¼ ë‹¨ìœ„ ë¦¬ìƒ˜í”Œ\n",
    "    sku_series = (\n",
    "        sku_df.set_index(date_column)[target_column]\n",
    "        .resample('W')  # ì£¼ ë‹¨ìœ„\n",
    "        .sum()          # ê·¸ ì£¼ì˜ ì´ ì˜ˆì • ìˆ˜ì£¼ëŸ‰\n",
    "    )\n",
    "\n",
    "    # ì™„ì „íˆ 0ë§Œ ìˆëŠ” SKUë„ ì¼ë‹¨ ê·¸ëŒ€ë¡œ ë‘  (ë‚˜ì¤‘ì— z-scoreì—ì„œ í‘œì¤€í¸ì°¨ 0ìœ¼ë¡œ ì²˜ë¦¬)\n",
    "    sku_weekly_sum[sku] = sku_series\n",
    "\n",
    "print(f\"2.4 SKU ê°œìˆ˜ (trainì—ì„œ ê´€ì¸¡ëœ): {len(sku_weekly_sum)}\")\n",
    "\n",
    "# 2.4 ê´€ì¸¡ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì€ SKUëŠ” ì œì™¸ í›„ë³´ë¡œ í‘œì‹œ\n",
    "#     ì˜ˆ: ê´€ì¸¡ ì£¼ì°¨ ìˆ˜ê°€ 3ì£¼ ë¯¸ë§Œì´ë©´ \"íŒ¨í„´\"ì´ë¼ê³  ë¶€ë¥´ê¸° ì–´ë µë‹¤ê³  ê°€ì •\n",
    "min_weeks_required = 3\n",
    "\n",
    "valid_skus = []\n",
    "lowdata_skus = []\n",
    "\n",
    "for sku, series in sku_weekly_sum.items():\n",
    "    # ìœ íš¨ ê´€ì¸¡ ì£¼(= ê°’ì´ ì „ë¶€ 0ì´ì–´ë„ ì£¼ ë‹¨ìœ„ë¡œ ì¡´ì¬í•˜ë©´ countë¡œ ì¡í˜)\n",
    "    observed_weeks = series.shape[0]\n",
    "    if observed_weeks < min_weeks_required:\n",
    "        lowdata_skus.append(sku)\n",
    "    else:\n",
    "        valid_skus.append(sku)\n",
    "\n",
    "print(f\"2.5 ìœ íš¨ SKU(ì£¼ ë‹¨ìœ„ {min_weeks_required}ì£¼ ì´ìƒ): {len(valid_skus)}ê°œ\")\n",
    "print(f\"    ê´€ì¸¡ ì ì–´ì„œ ì œì™¸ëœ SKU: {len(lowdata_skus)}ê°œ (ì˜ˆ: {lowdata_skus[:5]})\")\n",
    "\n",
    "# 2.6 z-score ì •ê·œí™” (valid_skusë§Œ)\n",
    "#     - ê° SKU ì‹œê³„ì—´ì„ ìê¸° ìì‹ ì˜ í‰ê· /í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”\n",
    "#     - í‘œì¤€í¸ì°¨ê°€ 0ì´ë©´ ì „ë¶€ 0ìœ¼ë¡œ (ì™„ì „ ì•ˆì • SKUë¡œ ê°„ì£¼)\n",
    "sku_weekly_z = {}\n",
    "\n",
    "for sku in valid_skus:\n",
    "    series = sku_weekly_sum[sku].astype(float)\n",
    "\n",
    "    mean_val = series.mean()\n",
    "    std_val = series.std(ddof=0)  # ëª¨ë¶„ì‚° ê¸°ì¤€ (ddof=0), ì•ˆì •ì ìœ¼ë¡œ 0 ì²´í¬ ê°€ëŠ¥\n",
    "\n",
    "    if std_val == 0 or np.isclose(std_val, 0.0):\n",
    "        z_series = pd.Series(\n",
    "            np.zeros_like(series.values, dtype=float),\n",
    "            index=series.index\n",
    "        )\n",
    "    else:\n",
    "        z_series = (series - mean_val) / std_val\n",
    "\n",
    "    sku_weekly_z[sku] = z_series\n",
    "\n",
    "print(f\"2.7 z-score ë³€í™˜ ì™„ë£Œ (ìœ íš¨ SKU {len(sku_weekly_z)}ê°œ)\")\n",
    "\n",
    "# 2.8 ê¸¸ì´ í‘œì¤€í™”(íŒ¨ë”©/íŠ¸ë¦¬ë°)ëŠ” ì•„ì§ ì•ˆ í•¨\n",
    "#     ì§€ê¸ˆì€ ê° SKUë³„ë¡œ ì£¼ì°¨ ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ\n",
    "#     ë‹¤ìŒ ë‹¨ê³„(3ë‹¨ê³„)ì—ì„œ DTWë¡œ ê±°ë¦¬ë¥¼ ê³„ì‚°í•  ë•Œ, DTWëŠ” ê¸¸ì´ê°€ ë‹¬ë¼ë„ ê´œì°®ìœ¼ë‹ˆê¹Œ\n",
    "#     ê¸¸ì´ ë§ì¶œ í•„ìš” ì—†ì´ ê·¸ëŒ€ë¡œ distance matrix ê³„ì‚° ê°€ëŠ¥.\n",
    "\n",
    "# 2.9 ìš”ì•½ í”„ë¦°íŠ¸\n",
    "example_sku = valid_skus[0] if valid_skus else None\n",
    "if example_sku is not None:\n",
    "    print(\"\\n[ìƒ˜í”Œ SKU 1ê°œ weekly ì‹œê³„ì—´ (ì›ë³¸ í•©ê³„)]\")\n",
    "    print(sku_weekly_sum[example_sku].head())\n",
    "\n",
    "    print(\"\\n[ìƒ˜í”Œ SKU 1ê°œ weekly ì‹œê³„ì—´ (z-score ì •ê·œí™”)]\")\n",
    "    print(sku_weekly_z[example_sku].head())\n",
    "\n",
    "print(\"\\nâœ… 2ë‹¨ê³„ ì™„ë£Œ:\")\n",
    "print(\" - sku_weekly_sum: SKUë³„ ì£¼ ë‹¨ìœ„ ìˆ˜ìš” í•©ê³„ ì‹œê³„ì—´ (trainë§Œ ì‚¬ìš©)\")\n",
    "print(\" - sku_weekly_z:   DTWìš©ìœ¼ë¡œ z-score ì •ê·œí™”ëœ ì‹œê³„ì—´ (ê¸¸ì´ ë‹¬ë¼ë„ OK)\")\n",
    "print(\" - valid_skus:     DTW í´ëŸ¬ìŠ¤í„°ë§ í›„ë³´ SKU ëª©ë¡\")\n",
    "print(\" - lowdata_skus:   ê´€ì¸¡ ì ì–´ì„œ cluster_id = -1 í›„ë³´\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b078db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [3ë‹¨ê³„: DTW ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°] ---\n",
      "3.2 í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ìƒ SKU ìˆ˜: 117\n",
      "3.5 DTW ê±°ë¦¬ í–‰ë ¬ ê³„ì‚° ì™„ë£Œ.\n",
      "    dist_matrix shape: (117, 117)\n",
      "    dist_matrix[0:3, 0:3] ì˜ˆì‹œ:\n",
      "[[ 0.          5.43735229 11.31886586]\n",
      " [ 5.43735229  0.         13.35943765]\n",
      " [11.31886586 13.35943765  0.        ]]\n",
      "3.6 ëŒ€ì¹­ ì—¬ë¶€: True\n",
      "    ìŒìˆ˜ ê±°ë¦¬ ì¡´ì¬?: False\n",
      "\n",
      "âœ… 3ë‹¨ê³„ ì™„ë£Œ:\n",
      " - sku_order: ê±°ë¦¬í–‰ë ¬ ì¸ë±ìŠ¤ì™€ SKUë¥¼ ë§¤í•‘í•  ë¦¬ìŠ¤íŠ¸\n",
      " - dist_matrix: SKU ê°„ DTW ê±°ë¦¬í–‰ë ¬ (shape = [N_SKU, N_SKU])\n",
      "ì´ì œ ì´ê±¸ ê°€ì§€ê³  k í›„ë³´ë³„ í´ëŸ¬ìŠ¤í„°ë§ê³¼ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë‹¤ìŒ ì…€ì—ì„œ ì§„í–‰).\n"
     ]
    }
   ],
   "source": [
    "# --- 3ë‹¨ê³„: DTW ê±°ë¦¬ í–‰ë ¬ ê³„ì‚° ---\n",
    "\n",
    "print(\"\\n--- [3ë‹¨ê³„: DTW ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°] ---\")\n",
    "\n",
    "# í•„ìˆ˜ ì „ì—­ ë³€ìˆ˜ë“¤ì´ ìˆëŠ”ì§€ ì²´í¬\n",
    "needed_vars = [\"sku_weekly_z\", \"valid_skus\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals() or globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"ğŸš¨ '{var_name}' ì´(ê°€) ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 2ë‹¨ê³„ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# 3.1 DTW í•¨ìˆ˜ ì •ì˜\n",
    "#     - í‘œì¤€ ë™ì  ê³„íšë²• êµ¬í˜„ (O(len(a)*len(b)))\n",
    "#     - ê±°ë¦¬ = |a_i - b_j| ì˜ ëˆ„ì  ìµœì†Œ í•© (L1 distance ê¸°ë°˜)\n",
    "def dtw_distance(seq_a: np.ndarray, seq_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    seq_a, seq_b: 1D numpy arrays (float)\n",
    "    return: DTW alignment cost (float)\n",
    "    \"\"\"\n",
    "    len_a = len(seq_a)\n",
    "    len_b = len(seq_b)\n",
    "\n",
    "    # DP ë§¤íŠ¸ë¦­ìŠ¤ ì´ˆê¸°í™”: ë§¤ìš° í° ê°’ìœ¼ë¡œ ì±„ì›€\n",
    "    dp = np.full((len_a + 1, len_b + 1), np.inf, dtype=float)\n",
    "    dp[0, 0] = 0.0\n",
    "\n",
    "    # ë™ì  ê³„íš: ì‚½ì…/ì‚­ì œ/ëŒ€ê°ì„  ì´ë™ ì¤‘ ìµœì†Œ ë¹„ìš© ê²½ë¡œ ëˆ„ì \n",
    "    for i in range(1, len_a + 1):\n",
    "        ai = seq_a[i - 1]\n",
    "        for j in range(1, len_b + 1):\n",
    "            bj = seq_b[j - 1]\n",
    "            cost = abs(ai - bj)\n",
    "\n",
    "            dp[i, j] = cost + min(\n",
    "                dp[i - 1, j],     # ì‚­ì œ (aì—ì„œ í•œ ê±¸ìŒ)\n",
    "                dp[i, j - 1],     # ì‚½ì… (bì—ì„œ í•œ ê±¸ìŒ)\n",
    "                dp[i - 1, j - 1]  # ë§¤ì¹­ (ëŒ€ê°ì„ )\n",
    "            )\n",
    "    return float(dp[len_a, len_b])\n",
    "\n",
    "# 3.2 sku_weekly_zì—ì„œ ì‚¬ìš©í•  SKU ìˆœì„œ ê³ ì •\n",
    "#     - valid_skusëŠ” ìš°ë¦¬ê°€ 2ë‹¨ê³„ì—ì„œ ê´€ì¸¡ ê¸°ê°„ ì¶©ë¶„í•˜ë‹¤ê³  íŒì •í•œ SKU ëª©ë¡\n",
    "sku_order = list(valid_skus)\n",
    "\n",
    "if len(sku_order) == 0:\n",
    "    raise RuntimeError(\"ğŸš¨ valid_skusê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ SKUê°€ ì—†ìœ¼ë©´ í´ëŸ¬ìŠ¤í„°ë§ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"3.2 í´ëŸ¬ìŠ¤í„°ë§ ëŒ€ìƒ SKU ìˆ˜: {len(sku_order)}\")\n",
    "\n",
    "# 3.3 ê° SKUì˜ ì‹œê³„ì—´ì„ numpy arrayë¡œ ë³€í™˜í•´ì„œ ìºì‹œ\n",
    "#     - ì‹œê³„ì—´ index(ì£¼ì°¨ ë‚ ì§œ)ëŠ” DTWì—ì„œ ì§ì ‘ ì•ˆ ì“°ê³  ê°’ ë²¡í„°ë§Œ ì“´ë‹¤.\n",
    "sku_series_cache = {}\n",
    "for sku in sku_order:\n",
    "    series_z = sku_weekly_z[sku]\n",
    "    # series_zëŠ” pandas Series (index=ì£¼ë³„ ë‚ ì§œ, values=z-scoreëœ ìˆ˜ìš”)\n",
    "    sku_series_cache[sku] = series_z.to_numpy(dtype=float)\n",
    "\n",
    "# 3.4 DTW ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°\n",
    "n = len(sku_order)\n",
    "dist_matrix = np.zeros((n, n), dtype=float)\n",
    "\n",
    "for i in range(n):\n",
    "    seq_i = sku_series_cache[sku_order[i]]\n",
    "    for j in range(i + 1, n):\n",
    "        seq_j = sku_series_cache[sku_order[j]]\n",
    "\n",
    "        d = dtw_distance(seq_i, seq_j)\n",
    "        dist_matrix[i, j] = d\n",
    "        dist_matrix[j, i] = d\n",
    "\n",
    "# ëŒ€ê°ì„ ì€ ìê¸° ìì‹ ì´ë¼ 0 ê·¸ëŒ€ë¡œ ë‘ë©´ ë¨\n",
    "\n",
    "print(\"3.5 DTW ê±°ë¦¬ í–‰ë ¬ ê³„ì‚° ì™„ë£Œ.\")\n",
    "print(f\"    dist_matrix shape: {dist_matrix.shape}\")\n",
    "print(\"    dist_matrix[0:3, 0:3] ì˜ˆì‹œ:\")\n",
    "print(dist_matrix[0:3, 0:3])\n",
    "\n",
    "# 3.6 ê°„ë‹¨ sanity check:\n",
    "#     - ëŒ€ì¹­ ì—¬ë¶€\n",
    "#     - ìŒìˆ˜ ê±°ë¦¬ ì—†ëŠ”ì§€\n",
    "is_symmetric = np.allclose(dist_matrix, dist_matrix.T, atol=1e-12)\n",
    "has_negative = (dist_matrix < 0).any()\n",
    "\n",
    "print(f\"3.6 ëŒ€ì¹­ ì—¬ë¶€: {is_symmetric}\")\n",
    "print(f\"    ìŒìˆ˜ ê±°ë¦¬ ì¡´ì¬?: {has_negative}\")\n",
    "\n",
    "if not is_symmetric:\n",
    "    raise RuntimeError(\"ğŸš¨ dist_matrixê°€ ëŒ€ì¹­ì´ ì•„ë‹™ë‹ˆë‹¤. DTW ê³„ì‚° ë£¨í”„ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "if has_negative:\n",
    "    raise RuntimeError(\"ğŸš¨ dist_matrixì— ìŒìˆ˜ ê±°ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. DTW distance ì •ì˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "print(\"\\nâœ… 3ë‹¨ê³„ ì™„ë£Œ:\")\n",
    "print(\" - sku_order: ê±°ë¦¬í–‰ë ¬ ì¸ë±ìŠ¤ì™€ SKUë¥¼ ë§¤í•‘í•  ë¦¬ìŠ¤íŠ¸\")\n",
    "print(\" - dist_matrix: SKU ê°„ DTW ê±°ë¦¬í–‰ë ¬ (shape = [N_SKU, N_SKU])\")\n",
    "print(\"ì´ì œ ì´ê±¸ ê°€ì§€ê³  k í›„ë³´ë³„ í´ëŸ¬ìŠ¤í„°ë§ê³¼ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë‹¤ìŒ ì…€ì—ì„œ ì§„í–‰).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e90352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [4ë‹¨ê³„: k ìµœì í™” ë° cluster_map ìƒì„± - Agglomerative v2] ---\n",
      "\n",
      "4.2 k í›„ë³´ë³„ ì‹¤ë£¨ì—£ ì ìˆ˜:\n",
      "   k=2: silhouette=0.1330377594277968\n",
      "   k=3: silhouette=0.12959370372021664\n",
      "   k=4: silhouette=0.16314839049493565\n",
      "   k=5: silhouette=0.17174521232688753\n",
      "\n",
      "4.3 ì„ íƒëœ best_k = 5 (silhouette=0.17174521232688753)\n",
      "\n",
      "4.4 ìµœì¢… cluster_map_df ì˜ˆì‹œ (ìƒìœ„ 10ê°œ):\n",
      "  Product_Number  cluster_id\n",
      "0     Product_84           1\n",
      "1     Product_85           1\n",
      "2     Product_86           0\n",
      "3     Product_87           0\n",
      "4     Product_88           1\n",
      "5     Product_89           0\n",
      "6     Product_8a           0\n",
      "7     Product_8b           1\n",
      "8     Product_8c           1\n",
      "9     Product_8d           1\n",
      "\n",
      "âœ… 4ë‹¨ê³„ ì™„ë£Œ:\n",
      " - best_k = 5, silhouette = 0.17174521232688753\n",
      " - cluster_map_df shape: (117, 2)\n",
      "   ì´ì œ cluster_map_dfë¥¼ train/test ì „ì²´ì— mergeí•´ì„œ cluster_idë¼ëŠ” ë²”ì£¼í˜• í”¼ì²˜ë¥¼ ë¶™ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# --- 4ë‹¨ê³„: k í›„ë³´ë³„ í´ëŸ¬ìŠ¤í„°ë§ (Agglomerative v2), ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°, best_k ì„ íƒ, cluster_map ìƒì„± ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print(\"\\n--- [4ë‹¨ê³„: k ìµœì í™” ë° cluster_map ìƒì„± - Agglomerative v2] ---\")\n",
    "\n",
    "# ì „ ë‹¨ê³„ ì‚°ì¶œë¬¼ í•„ìš”\n",
    "needed_vars = [\"dist_matrix\", \"sku_order\", \"lowdata_skus\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals():\n",
    "        raise RuntimeError(f\"ğŸš¨ '{var_name}' ì´(ê°€) ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    if globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"ğŸš¨ '{var_name}' ì´(ê°€) None ì…ë‹ˆë‹¤. ì´ì „ ì…€ì—ì„œ ìƒì„±ì´ ì•ˆ ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# í›„ë³´ k ê°’ë“¤\n",
    "k_candidates = [2, 3, 4, 5]\n",
    "\n",
    "silhouette_results = []\n",
    "label_results = {}\n",
    "\n",
    "for k in k_candidates:\n",
    "    if k >= len(sku_order):\n",
    "        print(f\"âš  k={k} ëŠ” SKU ìˆ˜ë³´ë‹¤ ë§ê±°ë‚˜ ê°™ì•„ì„œ ìŠ¤í‚µí•©ë‹ˆë‹¤.\")\n",
    "        continue\n",
    "\n",
    "    # AgglomerativeClustering (ì‹ í˜• sklearn API)\n",
    "    # - metric='precomputed' ë¡œ DTW ê±°ë¦¬í–‰ë ¬ ì‚¬ìš©\n",
    "    # - linkage='average' ë¡œ êµ°ì§‘ ê°„ ê±°ë¦¬ ê³„ì‚°\n",
    "    clusterer = AgglomerativeClustering(\n",
    "        n_clusters=k,\n",
    "        metric='precomputed',\n",
    "        linkage='average'\n",
    "    )\n",
    "\n",
    "    # fit_predictì— ê±°ë¦¬í–‰ë ¬(dist_matrix)ì„ ê·¸ëŒ€ë¡œ ë„£ëŠ”ë‹¤.\n",
    "    labels = clusterer.fit_predict(dist_matrix)\n",
    "\n",
    "    # silhouette_score ê³„ì‚°\n",
    "    # silhouette_score(X, labels, metric='precomputed')\n",
    "    # XëŠ” ê±°ë¦¬í–‰ë ¬\n",
    "    try:\n",
    "        sil = silhouette_score(dist_matrix, labels, metric=\"precomputed\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  k={k} ì‹¤ë£¨ì—£ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "        sil = None\n",
    "\n",
    "    silhouette_results.append({\n",
    "        \"k\": k,\n",
    "        \"silhouette\": sil\n",
    "    })\n",
    "    label_results[k] = labels\n",
    "\n",
    "# 4.2 kë³„ ì‹¤ë£¨ì—£ ì ìˆ˜ ì¶œë ¥\n",
    "print(\"\\n4.2 k í›„ë³´ë³„ ì‹¤ë£¨ì—£ ì ìˆ˜:\")\n",
    "for res in silhouette_results:\n",
    "    print(f\"   k={res['k']}: silhouette={res['silhouette']}\")\n",
    "\n",
    "# 4.3 best_k ì„ íƒ\n",
    "valid_scores = [\n",
    "    (res[\"k\"], res[\"silhouette\"])\n",
    "    for res in silhouette_results\n",
    "    if res[\"silhouette\"] is not None\n",
    "]\n",
    "\n",
    "if len(valid_scores) == 0:\n",
    "    raise RuntimeError(\"ğŸš¨ ëª¨ë“  kì— ëŒ€í•´ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì‹¤ë£¨ì—£ì´ ë†’ì€ kë¥¼ ìš°ì„ , ë™ë¥ ì´ë©´ ë” ì‘ì€ kë¥¼ ì„ íƒ\n",
    "best_k, best_sil = sorted(\n",
    "    valid_scores,\n",
    "    key=lambda x: (-x[1], x[0])\n",
    ")[0]\n",
    "\n",
    "print(f\"\\n4.3 ì„ íƒëœ best_k = {best_k} (silhouette={best_sil})\")\n",
    "\n",
    "best_labels = label_results[best_k]\n",
    "\n",
    "# 4.4 cluster_map_df ìƒì„±\n",
    "cluster_map_df = pd.DataFrame({\n",
    "    \"Product_Number\": sku_order,\n",
    "    \"cluster_id\": best_labels\n",
    "})\n",
    "\n",
    "# ê´€ì¸¡ ë¶€ì¡± SKU(-1) ì²˜ë¦¬ (lowdata_skusëŠ” 2ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘)\n",
    "if lowdata_skus:\n",
    "    low_df = pd.DataFrame({\n",
    "        \"Product_Number\": lowdata_skus,\n",
    "        \"cluster_id\": [-1] * len(lowdata_skus)\n",
    "    })\n",
    "    cluster_map_df = pd.concat([cluster_map_df, low_df], ignore_index=True)\n",
    "\n",
    "# í˜¹ì‹œë¼ë„ ì¤‘ë³µëœ Product_Numberê°€ ìƒê¸°ë©´ ë§ˆì§€ë§‰ì— ì •ë¦¬\n",
    "cluster_map_df = cluster_map_df.drop_duplicates(subset=[\"Product_Number\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n4.4 ìµœì¢… cluster_map_df ì˜ˆì‹œ (ìƒìœ„ 10ê°œ):\")\n",
    "print(cluster_map_df.head(10))\n",
    "\n",
    "print(f\"\\nâœ… 4ë‹¨ê³„ ì™„ë£Œ:\")\n",
    "print(f\" - best_k = {best_k}, silhouette = {best_sil}\")\n",
    "print(f\" - cluster_map_df shape: {cluster_map_df.shape}\")\n",
    "print(\"   ì´ì œ cluster_map_dfë¥¼ train/test ì „ì²´ì— mergeí•´ì„œ cluster_idë¼ëŠ” ë²”ì£¼í˜• í”¼ì²˜ë¥¼ ë¶™ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b05bfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [5ë‹¨ê³„: cluster_id ë¨¸ì§€ & ìµœì¢… í”¼ì²˜ì…‹ ìƒì„±] ---\n",
      "\n",
      "5.5 df_with_cluster ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5í–‰):\n",
      "  Product_Number       Date  Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰  cluster_id __SPLIT__\n",
      "0     Product_84 2022-01-26          0           1     train\n",
      "1     Product_85 2022-01-26          0           1     train\n",
      "2     Product_86 2022-01-26        616           0     train\n",
      "3     Product_87 2022-01-26        104           0     train\n",
      "4     Product_88 2022-01-26        187           1     train\n",
      "\n",
      "5.6 cluster_id ë¶„í¬ (value_counts):\n",
      "cluster_id\n",
      "1    4288\n",
      "0    3674\n",
      "2    1874\n",
      "4     760\n",
      "3      28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… 5ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… í”¼ì²˜ì…‹ì„ '../../data/dtw_clustering.csv'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "   ìµœì¢… shape: (10624, 21)\n",
      "   ì´ì œ ì´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  LightGBM ì‹¤í—˜ (baseline vs baseline+ts vs baseline+ts+cluster_id)ìœ¼ë¡œ MAE ë¹„êµë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# --- 5ë‹¨ê³„: cluster_idë¥¼ train/test ì „ì²´ì— mergeí•˜ê³  ìµœì¢… CSVë¡œ ì €ì¥ ---\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- [5ë‹¨ê³„: cluster_id ë¨¸ì§€ & ìµœì¢… í”¼ì²˜ì…‹ ìƒì„±] ---\")\n",
    "\n",
    "# ì „ ë‹¨ê³„ ì‚°ì¶œë¬¼ ì ê²€\n",
    "needed_vars = [\"df_train\", \"df_test\", \"cluster_map_df\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals() or globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"ğŸš¨ '{var_name}' ì´(ê°€) ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "\n",
    "date_column = 'Date'\n",
    "product_column = 'Product_Number'\n",
    "target_column = 'Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰'\n",
    "\n",
    "# ì¶œë ¥ ê²½ë¡œ (ìµœì¢… ì‚°ì¶œë¬¼)\n",
    "output_final_csv_file = '../../data/dtw_clustering.csv'\n",
    "\n",
    "# 5.1 train/testì— split í”Œë˜ê·¸ ë¶™ì´ê¸°\n",
    "train_labeled = df_train.copy()\n",
    "train_labeled[\"__SPLIT__\"] = \"train\"\n",
    "\n",
    "test_labeled = df_test.copy()\n",
    "test_labeled[\"__SPLIT__\"] = \"test\"\n",
    "\n",
    "# 5.2 cluster_id merge (ì™¼ìª½ ê¸°ì¤€: train/test, ì˜¤ë¥¸ìª½ ê¸°ì¤€: cluster_map_df)\n",
    "train_merged = pd.merge(\n",
    "    train_labeled,\n",
    "    cluster_map_df,\n",
    "    how=\"left\",\n",
    "    left_on=product_column,\n",
    "    right_on=\"Product_Number\"\n",
    ")\n",
    "\n",
    "test_merged = pd.merge(\n",
    "    test_labeled,\n",
    "    cluster_map_df,\n",
    "    how=\"left\",\n",
    "    left_on=product_column,\n",
    "    right_on=\"Product_Number\"\n",
    ")\n",
    "\n",
    "# merge í›„ ì¤‘ë³µëœ Product_Number ì»¬ëŸ¼ ì •ë¦¬\n",
    "# (left_on=Product_Number / right_on=Product_Number ì´ë¼ ë™ì¼ ì´ë¦„ì´ ìƒê¸¸ ìˆ˜ ìˆì–´ì„œ)\n",
    "if \"Product_Number_y\" in train_merged.columns:\n",
    "    # ì •ë¦¬: Product_Number_x -> Product_Number, drop Product_Number_y\n",
    "    train_merged.drop(columns=[\"Product_Number_y\"], inplace=True, errors=\"ignore\")\n",
    "    train_merged.rename(columns={\"Product_Number_x\": \"Product_Number\"}, inplace=True)\n",
    "\n",
    "if \"Product_Number_y\" in test_merged.columns:\n",
    "    test_merged.drop(columns=[\"Product_Number_y\"], inplace=True, errors=\"ignore\")\n",
    "    test_merged.rename(columns={\"Product_Number_x\": \"Product_Number\"}, inplace=True)\n",
    "\n",
    "# 5.3 cluster_id ëˆ„ë½ ì²´í¬\n",
    "missing_train = train_merged[\"cluster_id\"].isna().sum()\n",
    "missing_test  = test_merged[\"cluster_id\"].isna().sum()\n",
    "\n",
    "if missing_train > 0 or missing_test > 0:\n",
    "    print(f\"âš  ê²½ê³ : cluster_id ë§¤ì¹­ ì•ˆ ëœ í–‰ì´ ìˆìŠµë‹ˆë‹¤. train {missing_train}ê°œ, test {missing_test}ê°œ\")\n",
    "    # ì´ ìƒí™©ì´ë©´ ìš°ë¦¬ê°€ ëª¨ë¥´ëŠ” SKUê°€ testì— ìƒˆë¡œ ë“±ì¥í–ˆì„ ê°€ëŠ¥ì„±.\n",
    "    # ê·¸ SKUëŠ” í´ëŸ¬ìŠ¤í„°ë§ ë‹¹ì‹œ(train ê¸°ì¤€) íŒ¨í„´ì´ ì—†ì—ˆì„ ìˆ˜ë„ ìˆìŒ.\n",
    "    # ì´ëŸ° SKUëŠ” cluster_id = -1ìœ¼ë¡œ ë°€ì–´ë„£ëŠ” ê²Œ ì•ˆì „í•˜ë‹¤.\n",
    "    train_merged[\"cluster_id\"] = train_merged[\"cluster_id\"].fillna(-1)\n",
    "    test_merged[\"cluster_id\"]  = test_merged[\"cluster_id\"].fillna(-1)\n",
    "\n",
    "# 5.4 ìµœì¢… df ê²°í•©\n",
    "df_with_cluster = pd.concat([train_merged, test_merged], ignore_index=True)\n",
    "\n",
    "# 5.5 sanity check í”„ë¦°íŠ¸\n",
    "print(\"\\n5.5 df_with_cluster ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5í–‰):\")\n",
    "print(df_with_cluster[[product_column, date_column, target_column, \"cluster_id\", \"__SPLIT__\"]].head())\n",
    "\n",
    "print(\"\\n5.6 cluster_id ë¶„í¬ (value_counts):\")\n",
    "print(df_with_cluster[\"cluster_id\"].value_counts())\n",
    "\n",
    "# 5.7 ì €ì¥\n",
    "# ì´ df_with_clusterëŠ” \"ê³µí†µ ì „ì²˜ë¦¬ + (ê¸°ì¡´ í”¼ì²˜ë“¤) + cluster_id\"ê°€ ë¶™ì€ ìµœì¢… í”¼ì²˜ì…‹ì´ë‹¤.\n",
    "# ë‹¤ìŒ ë‹¨ê³„(LightGBM)ì—ì„œ ë°”ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.\n",
    "os.makedirs(os.path.dirname(output_final_csv_file), exist_ok=True)\n",
    "df_with_cluster.to_csv(output_final_csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nâœ… 5ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… í”¼ì²˜ì…‹ì„ '{output_final_csv_file}'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"   ìµœì¢… shape: {df_with_cluster.shape}\")\n",
    "print(\"   ì´ì œ ì´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  LightGBM ì‹¤í—˜ (baseline vs baseline+ts vs baseline+ts+cluster_id)ìœ¼ë¡œ MAE ë¹„êµë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [5ë‹¨ê³„: cluster_id ë¨¸ì§€ & ìµœì¢… í”¼ì²˜ì…‹ ìƒì„±] ---\n",
      "\n",
      "5.5 df_with_cluster ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5í–‰):\n",
      "   Product_Number       Date  Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰  cluster_id __SPLIT__\n",
      "0               0 2022-01-26          0           1     train\n",
      "1               1 2022-01-26          0           1     train\n",
      "2               2 2022-01-26        616           0     train\n",
      "3               3 2022-01-26        104           0     train\n",
      "4               4 2022-01-26        187           1     train\n",
      "\n",
      "5.6 cluster_id ë¶„í¬ (value_counts):\n",
      "cluster_id\n",
      "1    4288\n",
      "0    3674\n",
      "2    1874\n",
      "4     760\n",
      "3      28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… 5ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… í”¼ì²˜ì…‹ì„ '../../data/dtw_clustering.csv'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "   ìµœì¢… shape: (10624, 21)\n",
      "   ì´ì œ ì´ ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ LightGBM ì‹¤í—˜ (baseline vs cluster_id í¬í•¨)ìœ¼ë¡œ test MAE ë¹„êµë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# --- 5ë‹¨ê³„: cluster_idë¥¼ train/test ì „ì²´ì— mergeí•˜ê³  ìµœì¢… CSVë¡œ ì €ì¥ ---\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- [5ë‹¨ê³„: cluster_id ë¨¸ì§€ & ìµœì¢… í”¼ì²˜ì…‹ ìƒì„±] ---\")\n",
    "\n",
    "# ì „ ë‹¨ê³„ ì‚°ì¶œë¬¼ ì ê²€\n",
    "needed_vars = [\"df_train\", \"df_test\", \"cluster_map_df\"]\n",
    "for var_name in needed_vars:\n",
    "    if var_name not in globals() or globals()[var_name] is None:\n",
    "        raise RuntimeError(f\"ğŸš¨ '{var_name}' ì´(ê°€) ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "\n",
    "date_column = 'Date'\n",
    "product_column = 'Product_Number'\n",
    "target_column = 'Tì¼ ì˜ˆì • ìˆ˜ì£¼ëŸ‰'\n",
    "\n",
    "# ìµœì¢… ì‚°ì¶œ íŒŒì¼ ê²½ë¡œ\n",
    "output_final_csv_file = '../../data/dtw_clustering.csv'\n",
    "\n",
    "# 5.1 train/testì— split í”Œë˜ê·¸ ë¶™ì´ê¸°\n",
    "train_labeled = df_train.copy()\n",
    "train_labeled[\"__SPLIT__\"] = \"train\"\n",
    "\n",
    "test_labeled = df_test.copy()\n",
    "test_labeled[\"__SPLIT__\"] = \"test\"\n",
    "\n",
    "# 5.2 cluster_id merge (ì™¼ìª½ = train/test, ì˜¤ë¥¸ìª½ = cluster_map_df)\n",
    "train_merged = pd.merge(\n",
    "    train_labeled,\n",
    "    cluster_map_df,\n",
    "    how=\"left\",\n",
    "    left_on=product_column,\n",
    "    right_on=\"Product_Number\"\n",
    ")\n",
    "\n",
    "test_merged = pd.merge(\n",
    "    test_labeled,\n",
    "    cluster_map_df,\n",
    "    how=\"left\",\n",
    "    left_on=product_column,\n",
    "    right_on=\"Product_Number\"\n",
    ")\n",
    "\n",
    "# ë³‘í•© í›„ ì¤‘ë³µë˜ëŠ” Product_Number_* ì •ë¦¬\n",
    "if \"Product_Number_y\" in train_merged.columns:\n",
    "    train_merged.drop(columns=[\"Product_Number_y\"], inplace=True, errors=\"ignore\")\n",
    "    train_merged.rename(columns={\"Product_Number_x\": \"Product_Number\"}, inplace=True)\n",
    "\n",
    "if \"Product_Number_y\" in test_merged.columns:\n",
    "    test_merged.drop(columns=[\"Product_Number_y\"], inplace=True, errors=\"ignore\")\n",
    "    test_merged.rename(columns={\"Product_Number_x\": \"Product_Number\"}, inplace=True)\n",
    "\n",
    "# 5.3 cluster_id ëˆ„ë½ê°’ ì²˜ë¦¬\n",
    "missing_train = train_merged[\"cluster_id\"].isna().sum()\n",
    "missing_test  = test_merged[\"cluster_id\"].isna().sum()\n",
    "\n",
    "if missing_train > 0 or missing_test > 0:\n",
    "    print(f\"âš  ê²½ê³ : cluster_id ë§¤ì¹­ ì•ˆ ëœ í–‰ì´ ìˆìŠµë‹ˆë‹¤. train {missing_train}ê°œ, test {missing_test}ê°œ\")\n",
    "    # train ê¸°ê°„ì—” ì›ë˜ ë‹¤ ìˆì–´ì•¼ ì •ìƒì¸ë°, testì—ëŠ” ì‹ ê·œ SKUê°€ ë…¸ì¶œë  ìˆ˜ ìˆìŒ.\n",
    "    # ê·¸ëŸ° SKUëŠ” cluster_id = -1ë¡œ íƒœê¹…í•´ì„œ 'ë¯¸ë¶„ë¥˜/ì‹ ê·œ ë¦¬ìŠ¤í¬' ì·¨ê¸‰ ê°€ëŠ¥.\n",
    "    train_merged[\"cluster_id\"] = train_merged[\"cluster_id\"].fillna(-1)\n",
    "    test_merged[\"cluster_id\"]  = test_merged[\"cluster_id\"].fillna(-1)\n",
    "\n",
    "# 5.4 ìµœì¢… df ê²°í•©\n",
    "df_with_cluster = pd.concat([train_merged, test_merged], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# 5.5 sanity check ì¶œë ¥\n",
    "print(\"\\n5.5 df_with_cluster ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5í–‰):\")\n",
    "print(df_with_cluster[[product_column, date_column, target_column, \"cluster_id\", \"__SPLIT__\"]].head())\n",
    "\n",
    "print(\"\\n5.6 cluster_id ë¶„í¬ (value_counts):\")\n",
    "print(df_with_cluster[\"cluster_id\"].value_counts())\n",
    "\n",
    "# 5.7 ì €ì¥\n",
    "os.makedirs(os.path.dirname(output_final_csv_file), exist_ok=True)\n",
    "df_with_cluster.to_csv(output_final_csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nâœ… 5ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… í”¼ì²˜ì…‹ì„ '{output_final_csv_file}'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"   ìµœì¢… shape: {df_with_cluster.shape}\")\n",
    "print(\"   ì´ì œ ì´ ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ LightGBM ì‹¤í—˜ (baseline vs cluster_id í¬í•¨)ìœ¼ë¡œ \"\n",
    "      \"test MAE ë¹„êµë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
